---
title: Lower Bounds for Time-Varying Kernelized Bandits
openreview: hAQnKnkCT4
abstract: The optimization of black-box functions with noisy observations is a fundamental
  problem with widespread applications, and has been widely studied under the assumption
  that the function lies in a reproducing kernel Hilbert space (RKHS).  This problem
  has been studied extensively in the stationary setting, and near-optimal regret
  bounds are known via developments in both upper and lower bounds.  In this paper,
  we consider non-stationary scenarios, which are crucial for certain applications
  but are currently less well-understood.  Specifically, we provide the first algorithm-independent
  lower bounds, where the time variations are subject satisfying a total variation
  budget according to some function norm.  Under $\ell_{\infty}$-norm variations,
  our bounds are found to be close to an existing upper bound (Hong et al., 2023).  Under
  RKHS norm variations, the upper and lower bounds are still reasonably close but
  with more of a gap, raising the interesting open question of whether non-minor improvements
  in the upper bound are possible.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cai25a
month: 0
tex_title: Lower Bounds for Time-Varying Kernelized Bandits
firstpage: 73
lastpage: 81
page: 73-81
order: 73
cycles: false
bibtex_author: Cai, Xu and Scarlett, Jonathan
author:
- given: Xu
  family: Cai
- given: Jonathan
  family: Scarlett
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/cai25a/cai25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
