---
title: " InnerThoughts: Disentangling Representations and Predictions in Large Language
  Models "
software: " https://github.com/networkslab/innerthoughts "
openreview: " https://openreview.net/forum?id=H0ZBGst3Vh "
abstract: " Large language models (LLMs) contain substantial factual knowledge which
  is commonly elicited by multiple-choice question-answering prompts. Internally,
  such models process the prompt through multiple transformer layers, building varying
  representations of the problem within its hidden states. Ultimately, however, only
  the hidden state corresponding to the final layer and token position is used to
  predict the answer label. In this work, we propose instead to learn a small separate
  neural network predictor module on a collection of training questions, that take
  the hidden states from all the layers at the last temporal position as input and
  outputs predictions. In effect, such a framework disentangles the representational
  abilities of LLMs from their predictive abilities. On a collection of hard benchmarks,
  our method achieves considerable improvements in performance, sometimes comparable
  to supervised fine-tuning procedures, but at a fraction of the computational cost. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chetelat25a
month: 0
tex_title: " InnerThoughts: Disentangling Representations and Predictions in Large
  Language Models "
firstpage: 3862
lastpage: 3870
page: 3862-3870
order: 3862
cycles: false
bibtex_author: Ch{\'e}telat, Didier and Cotnareanu, Joseph and Thompson, Rylee and
  Zhang, Yingxue and Coates, Mark
author:
- given: Didier
  family: Ch√©telat
- given: Joseph
  family: Cotnareanu
- given: Rylee
  family: Thompson
- given: Yingxue
  family: Zhang
- given: Mark
  family: Coates
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/chetelat25a/chetelat25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
