---
title: Minimum Empirical Divergence for Sub-Gaussian Linear Bandits
software: https://github.com/Kapilan-Balagopalan/Linear-Bandit-Algorithms
openreview: oYOzz3Fm7H
abstract: We propose a novel linear bandit algorithm called LinMED (Linear Minimum
  Empirical Divergence), which is a linear extension of the MED algorithm that was
  originally designed for multi-armed bandits. LinMED is a randomized algorithm that
  admits a closed-form computation of the arm sampling probabilities, unlike the popular
  randomized algorithm called linear Thompson sampling. Such a feature proves useful
  for off-policy evaluation where the unbiased evaluation requires accurately computing
  the sampling probability. We prove that LinMED enjoys a near-optimal regret bound
  of $d\sqrt{n}$ up to logarithmic factors where $d$ is the dimension and $n$ is the
  time horizon. We further show that LinMED enjoys a $\frac{d^2}{\Delta}\left(\log^2(n)\right)\log\left(\log(n)\right)$
  problem-dependent regret where $\Delta$ is the smallest suboptimality gap. Our empirical
  study shows that LinMED has a competitive performance with the state-of-the-art
  algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: balagopalan25a
month: 0
tex_title: Minimum Empirical Divergence for Sub-Gaussian Linear Bandits
firstpage: 1585
lastpage: 1593
page: 1585-1593
order: 1585
cycles: false
bibtex_author: Balagopalan, Kapilan and Jun, Kwang-Sung
author:
- given: Kapilan
  family: Balagopalan
- given: Kwang-Sung
  family: Jun
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/balagopalan25a/balagopalan25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
