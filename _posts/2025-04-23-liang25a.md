---
title: Looped ReLU MLPs May Be All You Need as Practical Programmable Computers
openreview: XjNFnBqrEi
abstract: Previous work has demonstrated that attention mechanisms are Turing complete.
  More recently, it has been shown that a looped 9-layer Transformer can function
  as a universal programmable computer. In contrast, the multi-layer perceptrons with
  $\mathsf{ReLU}$ activation ($\mathsf{ReLU}$-$\mathsf{MLP}$), one of the most fundamental
  components of neural networks, is known to be expressive; specifically, a two-layer
  neural network is a universal approximator given an exponentially large number of
  hidden neurons. However, it remains unclear whether a $\mathsf{ReLU}$-$\mathsf{MLP}$
  can be made into a universal programmable computer using a practical number of weights.
  In this work, we provide an affirmative answer that a looped 23-layer $\mathsf{ReLU}$-$\mathsf{MLP}$
  is capable of performing the basic necessary operations, more efficiently and effectively
  functioning as a programmable computer than a looped Transformer. This indicates
  simple modules have stronger expressive power than previously expected and have
  not been fully explored. Our work provides insights into the mechanisms of neural
  networks and demonstrates that complex tasks, such as functioning as a programmable
  computer, do not necessarily require advanced architectures like Transformers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liang25a
month: 0
tex_title: Looped ReLU MLPs May Be All You Need as Practical Programmable Computers
firstpage: 2647
lastpage: 2655
page: 2647-2655
order: 2647
cycles: false
bibtex_author: Liang, Yingyu and Sha, Zhizhou and Shi, Zhenmei and Song, Zhao and
  Zhou, Yufa
author:
- given: Yingyu
  family: Liang
- given: Zhizhou
  family: Sha
- given: Zhenmei
  family: Shi
- given: Zhao
  family: Song
- given: Yufa
  family: Zhou
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/liang25a/liang25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
