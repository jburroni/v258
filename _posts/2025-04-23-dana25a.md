---
title: " Memorization in Attention-only Transformers "
software: " https://github.com/leodana2000/Transformer_Attentional_Memory "
openreview: " https://openreview.net/forum?id=hyALFH7E9J "
abstract: " Recent research has explored the memorization capacity of multi-head attention,
  but these findings are constrained by unrealistic limitations on the context size.
  We present a novel proof for language-based Transformers that extends the current
  hypothesis to any context size. Our approach improves upon the state-of-the-art
  by achieving more effective exact memorization with an attention layer, while also
  introducing the concept of approximate memorization of distributions. Through experimental
  validation, we demonstrate that our proposed bounds more accurately reflect the
  true memorization capacity of language models, and provide a precise comparison
  with prior work. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dana25a
month: 0
tex_title: " Memorization in Attention-only Transformers "
firstpage: 3133
lastpage: 3141
page: 3133-3141
order: 3133
cycles: false
bibtex_author: Dana, L{\'e}o and Pydi, Muni Sreenivas and Chevaleyre, Yann
author:
- given: LÃ©o
  family: Dana
- given: Muni Sreenivas
  family: Pydi
- given: Yann
  family: Chevaleyre
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/dana25a/dana25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
