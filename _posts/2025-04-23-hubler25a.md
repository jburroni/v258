---
title: " From Gradient Clipping to Normalization for Heavy Tailed SGD "
software: " https://polybox.ethz.ch/index.php/s/6LONAuyBbiDMAFf/download "
openreview: " https://openreview.net/forum?id=nuG0FYyWRT "
abstract: " Recent empirical evidence indicates that many machine learning applications
  involve heavy-tailed gradient noise, which challenges the standard assumptions of
  bounded variance in stochastic optimization. Gradient clipping has emerged as a
  popular tool to handle this heavy-tailed noise, as it achieves good performance
  both theoretically and practically.  However, our current theoretical understanding
  of non-convex gradient clipping has three main shortcomings. First, the theory hinges
  on large, increasing clipping thresholds, which are in stark contrast to the small
  constant clipping thresholds employed in practice. Second, clipping thresholds require
  knowledge of problem-dependent parameters to guarantee convergence. Lastly, even
  with this knowledge, current sample complexity upper bounds for the method are sub-optimal
  in nearly all parameters.  To address these issues and motivated by practical observations,
  we make the connection of gradient clipping to its close relative — Normalized SGD
  (NSGD) — and study its convergence properties. First, we establish a parameter-free
  sample complexity for NSGD of $\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$
  to find an $\\varepsilon$-stationary point, only assuming a finite $p$-th central
  moment of the noise, $p\\in(1,2]$.  Furthermore, we prove the tightness of this
  result, by providing a matching algorithm-specific lower bound. In the setting where
  all problem parameters are known, we show this complexity is improved to $\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$,
  matching the previously known lower bound for all first-order methods in all problem
  dependent parameters.  Finally, we establish high-probability convergence of NSGD
  with a mild logarithmic dependence on the failure probability. Our work complements
  the studies of gradient clipping under heavy-tailed noise, improving the sample
  complexities of existing algorithms and offering an alternative mechanism to achieve
  high-probability convergence. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hubler25a
month: 0
tex_title: " From Gradient Clipping to Normalization for Heavy Tailed SGD "
firstpage: 2413
lastpage: 2421
page: 2413-2421
order: 2413
cycles: false
bibtex_author: H{\"u}bler, Florian and Fatkhullin, Ilyas and He, Niao
author:
- given: Florian
  family: Hübler
- given: Ilyas
  family: Fatkhullin
- given: Niao
  family: He
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/hubler25a/hubler25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
