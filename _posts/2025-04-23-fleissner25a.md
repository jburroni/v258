---
title: Infinite Width Limits of Self Supervised Neural Networks
software: https://colab.research.google.com/drive/12weqAhMLbv5KJ8MlUi80iaEjtDn8lMcv?usp=sharing
openreview: CTlnhuuiaP
abstract: The NTK is a widely used tool in the theoretical analysis of deep learning,
  allowing us to look at supervised deep neural networks through the lenses of kernel
  regression. Recently, several works have investigated kernel models for self-supervised
  learning, hypothesizing that these also shed light on the behavior of wide neural
  networks by virtue of the NTK. However, it remains an open question to what extent
  this connection is mathematically sound â€” it is a commonly encountered misbelief
  that the kernel behavior of wide neural networks emerges irrespective of the loss
  function it is trained on. In this paper, we bridge the gap between the NTK and
  self-supervised learning, focusing on two-layer neural networks trained under the
  Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant
  as the width of the network approaches infinity. Our analysis technique is a bit
  different from previous works on the NTK and may be of independent interest. Overall,
  our work provides a first justification for the use of classic kernel theory to
  understand self-supervised learning of wide neural networks. Building on this result,
  we derive generalization error bounds for kernelized Barlow Twins and connect them
  to neural networks of finite width.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fleissner25a
month: 0
tex_title: Infinite Width Limits of Self Supervised Neural Networks
firstpage: 3169
lastpage: 3177
page: 3169-3177
order: 3169
cycles: false
bibtex_author: Fleissner, Maximilian and Anil, Gautham Govind and Ghoshdastidar, Debarghya
author:
- given: Maximilian
  family: Fleissner
- given: Gautham Govind
  family: Anil
- given: Debarghya
  family: Ghoshdastidar
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/fleissner25a/fleissner25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
