---
title: " Training LLMs with MXFP4 "
software: " https://github.com/amazon-science/mxfp4-llm "
openreview: " https://openreview.net/forum?id=a8z5Q0WSPL "
abstract: " Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications
  (GEMMs) and reduce training costs.  However, directly using MXFP4 instead of BF16
  during training significantly degrades model quality.  In this work, we present
  the first near-lossless training recipe that uses MXFP4 GEMMs, which are $2\\times$
  faster than FP8 on supported hardware. Our key insight is to compute unbiased gradient
  estimates with stochastic rounding (SR), resulting in more accurate model updates.
  However, directly applying SR to MXFP4 can result in high variance from block-level
  outliers, harming convergence. To overcome this, we use the random Hadamard tranform
  to theoretically bound the variance of SR. We train GPT models up to 6.7B parameters
  and find that our method induces minimal degradation over mixed-precision BF16 training.
  Our recipe computes $>1/2$ the training FLOPs in MXFP4, enabling an estimated speedup
  of $>1.3\\times$ over FP8 and $>1.7\\times$ over BF16 during backpropagation. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tseng25a
month: 0
tex_title: " Training LLMs with MXFP4 "
firstpage: 1630
lastpage: 1638
page: 1630-1638
order: 1630
cycles: false
bibtex_author: Tseng, Albert and Yu, Tao and Park, Youngsuk
author:
- given: Albert
  family: Tseng
- given: Tao
  family: Yu
- given: Youngsuk
  family: Park
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/tseng25a/tseng25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
