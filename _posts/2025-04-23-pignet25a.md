---
title: Legitimate ground-truth-free metrics for deep uncertainty classification scoring
software: https://github.com/owkin/legitimate-uq-metrics
openreview: t7J3l7RwGA
abstract: Despite the increasing demand for safer machine learning practices, the
  use of Uncertainty Quantification (UQ) methods in production remains limited. This
  limitation is exacerbated by the challenge of validating UQ methods in absence of
  UQ ground truth.  In classification tasks, when only a usual set of test data is
  at hand, several authors suggested different metrics that can be computed from such
  test points while assessing the quality of quantified uncertainties.  This paper
  investigates such metrics and proves that they are theoretically well-behaved and
  actually tied to some uncertainty ground truth which is easily interpretable in
  terms of model prediction trustworthiness ranking.  Equipped with those new results,
  and given the applicability of those metrics in the usual supervised paradigm, we
  argue that our contributions will help promoting a broader use of UQ in deep learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pignet25a
month: 0
tex_title: Legitimate ground-truth-free metrics for deep uncertainty classification
  scoring
firstpage: 2197
lastpage: 2205
page: 2197-2205
order: 2197
cycles: false
bibtex_author: Pignet, Arthur and Regniez, Chiara and Klein, John
author:
- given: Arthur
  family: Pignet
- given: Chiara
  family: Regniez
- given: John
  family: Klein
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/pignet25a/pignet25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
