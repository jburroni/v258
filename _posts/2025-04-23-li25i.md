---
title: On the Power of Multitask Representation Learning with Gradient Descent
openreview: gY7Igf5Jwt
abstract: Representation learning, particularly multi-task representation learning,
  has gained widespread popularity in various deep learning applications, ranging
  from computer vision to natural language processing, due to its remarkable generalization
  performance. Despite its growing use, our understanding of the underlying mechanisms
  remains limited. In this paper, we provide a theoretical analysis elucidating why
  multi-task representation learning outperforms its single-task counterpart in scenarios
  involving over-parameterized two-layer convolutional neural networks trained by
  gradient descent. Our analysis is based on a data model that encompasses both task-shared
  and task-specific features, a setting commonly encountered in real-world applications.
  We also present experiments on synthetic and real-world data to illustrate and validate
  our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li25i
month: 0
tex_title: On the Power of Multitask Representation Learning with Gradient Descent
firstpage: 4357
lastpage: 4365
page: 4357-4365
order: 4357
cycles: false
bibtex_author: Li, Qiaobo and Chen, Zixiang and Deng, Yihe and Kou, Yiwen and Cao,
  Yuan and Gu, Quanquan
author:
- given: Qiaobo
  family: Li
- given: Zixiang
  family: Chen
- given: Yihe
  family: Deng
- given: Yiwen
  family: Kou
- given: Yuan
  family: Cao
- given: Quanquan
  family: Gu
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/li25i/li25i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
