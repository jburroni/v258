---
title: Gated Recurrent Neural Networks with Weighted Time-Delay Feedback
openreview: CgJiTbwwrY
abstract: In this paper, we present a novel approach to modeling long-term dependencies
  in sequential data by introducing a gated recurrent unit (GRU) with a weighted time-delay
  feedback mechanism. Our proposed model, named $\tau$-GRU, is a discretized version
  of a continuous-time formulation of a recurrent unit, where the dynamics are governed
  by delay differential equations (DDEs). We prove the existence and uniqueness of
  solutions for the continuous-time model and show that the proposed feedback mechanism
  can significantly improve the modeling of long-term dependencies. Our empirical
  results indicate that $\tau$-GRU outperforms state-of-the-art recurrent units and
  gated recurrent architectures on a range of tasks, achieving faster convergence
  and better generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: erichson25a
month: 0
tex_title: Gated Recurrent Neural Networks with Weighted Time-Delay Feedback
firstpage: 3646
lastpage: 3654
page: 3646-3654
order: 3646
cycles: false
bibtex_author: Erichson, N. Benjamin and Lim, Soon Hoe and Mahoney, Michael W.
author:
- given: N. Benjamin
  family: Erichson
- given: Soon Hoe
  family: Lim
- given: Michael W.
  family: Mahoney
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/erichson25a/erichson25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
