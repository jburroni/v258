---
title: " How Well Can Transformers Emulate In-Context Newton’s Method? "
software: " https://github.com/Leiay/icl_newton "
openreview: " https://openreview.net/forum?id=cj5L29VWol "
abstract: " Transformer-based models have demonstrated remarkable in-context learning
  capabilities, prompting extensive research into its underlying mechanisms. Recent
  studies have suggested that Transformers can implement first-order optimization
  algorithms for in-context learning and even second order ones for the case of linear
  regression. In this work, we study whether Transformers can perform higher order
  optimization methods, beyond the case of linear regression. We establish that linear
  attention Transformers with ReLU layers can approximate second order optimization
  algorithms for the task of logistic regression and achieve $\\epsilon$ error with
  only a logarithmic to the error more layers. Our results suggest the ability of
  the Transformer architecture to implement complex algorithms, beyond gradient descent. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: giannou25a
month: 0
tex_title: " How Well Can Transformers Emulate In-Context Newton’s Method? "
firstpage: 4843
lastpage: 4851
page: 4843-4851
order: 4843
cycles: false
bibtex_author: Giannou, Angeliki and Yang, Liu and Wang, Tianhao and Papailiopoulos,
  Dimitris and Lee, Jason D.
author:
- given: Angeliki
  family: Giannou
- given: Liu
  family: Yang
- given: Tianhao
  family: Wang
- given: Dimitris
  family: Papailiopoulos
- given: Jason D.
  family: Lee
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/giannou25a/giannou25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
