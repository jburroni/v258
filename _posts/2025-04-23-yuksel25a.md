---
title: On the Sample Complexity of Next-Token Prediction
openreview: eJkNMwzZzy
abstract: Next-token prediction with cross-entropy loss is the objective of choice
  in sequence and language modeling. Despite its widespread use, there is a lack of
  theoretical analysis regarding the generalization of models trained using this objective.
  In this work, we provide an analysis of empirical risk minimization for sequential
  inputs generated by order-$k$ Markov chains. Assuming bounded and Lipschitz logit
  functions, our results show that in-sample prediction error decays optimally with
  the number of tokens, whereas out-of-sample error incurs an additional term related
  to the mixing properties of the Markov chain. These rates depend on the statistical
  complexity of the hypothesis class and can lead to generalization errors that do
  not scale exponentially with the order of the Markov chain—unlike classical $k$-gram
  estimators. Finally, we discuss the possibility of achieving generalization rates
  independent of mixing.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yuksel25a
month: 0
tex_title: On the Sample Complexity of Next-Token Prediction
firstpage: 694
lastpage: 702
page: 694-702
order: 694
cycles: false
bibtex_author: Y{\"u}ksel, O{\u{g}}uz Kaan and Flammarion, Nicolas
author:
- given: Oğuz Kaan
  family: Yüksel
- given: Nicolas
  family: Flammarion
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/yuksel25a/yuksel25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
