---
title: High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient
  Descent under Heavy-tailed Noise
openreview: hsM969HT5W
abstract: We study high-probability convergence in online learning, in the presence
  of heavy-tailed noise. To combat the heavy tails, a general framework of nonlinear
  SGD methods is considered, subsuming several popular nonlinearities like sign, quantization,
  component-wise and joint clipping. In our work the nonlinearity is treated in a
  black-box manner, allowing us to establish unified guarantees for a broad range
  of nonlinear methods. For symmetric noise and non-convex costs we establish convergence
  of gradient norm-squared, at a rate $\widetilde{\mathcal{O}}(t^{-1/4})$, while for
  the last iterate of strongly convex costs we establish convergence to the population
  optima, at a rate $\mathcal{O}(t^{-\zeta})$, where $\zeta \in (0,1)$ depends on
  noise and problem parameters. Further, if the noise is a (biased) mixture of symmetric
  and non-symmetric components, we show convergence to a neighbourhood of stationarity,
  whose size depends on the mixture coefficient, nonlinearity and noise. Compared
  to state-of-the-art, who only consider clipping and require unbiased noise with
  bounded $p$-th moments, $p \in (1,2]$, we provide guarantees for a broad class of
  nonlinearities, without any assumptions on noise moments. While the rate exponents
  in state-of-the-art depend on noise moments and vanish as $p \rightarrow 1$, our
  exponents are constant and strictly better whenever $p < 6/5$ for non-convex and
  $p < 8/7$ for strongly convex costs. Experiments validate our theory, showing that
  clipping is not always the optimal nonlinearity, further underlining the value of
  a general framework.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: armacki25a
month: 0
tex_title: High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient
  Descent under Heavy-tailed Noise
firstpage: 1774
lastpage: 1782
page: 1774-1782
order: 1774
cycles: false
bibtex_author: Armacki, Aleksandar and Yu, Shuhua and Sharma, Pranay and Joshi, Gauri
  and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya
author:
- given: Aleksandar
  family: Armacki
- given: Shuhua
  family: Yu
- given: Pranay
  family: Sharma
- given: Gauri
  family: Joshi
- given: Dragana
  family: Bajovic
- given: Dusan
  family: Jakovetic
- given: Soummya
  family: Kar
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/armacki25a/armacki25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
