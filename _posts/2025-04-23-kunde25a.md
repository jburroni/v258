---
title: Transformers are Provably Optimal In-context Estimators for Wireless Communications
software: https://github.com/vishnutez/in-context-estimation
openreview: nhGtq5s6GJ
abstract: 'Pre-trained transformers exhibit the capability of adapting to new tasks
  through in-context learning (ICL), where they efficiently utilize a limited set
  of prompts without explicit model optimization. The canonical communication problem
  of estimating transmitted symbols from received observations can be modeled as an
  in-context learning problem: Received observations are a noisy function of transmitted
  symbols, and this function can be represented by an unknown parameter whose statistics
  depend on an unknown latent context. This problem, which we term in-context estimation
  (ICE), has significantly greater complexity than the extensively studied linear
  regression problem. The optimal solution to the ICE problem is a non-linear function
  of the underlying context. In this paper, we prove that, for a subclass of such
  problems, a single-layer softmax attention transformer (SAT) computes the optimal
  solution of the above estimation problem in the limit of large prompt length. We
  also prove that the optimal configuration of such a transformer is indeed the minimizer
  of the corresponding training loss. Further, we empirically demonstrate the proficiency
  of multi-layer transformers in efficiently solving broader in-context estimation
  problems. Through extensive simulations, we show that solving ICE problems using
  transformers significantly outperforms standard approaches. Moreover, just with
  a few context examples, it achieves the same performance as an estimator with perfect
  knowledge of the latent context.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kunde25a
month: 0
tex_title: Transformers are Provably Optimal In-context Estimators for Wireless Communications
firstpage: 1531
lastpage: 1539
page: 1531-1539
order: 1531
cycles: false
bibtex_author: Kunde, Vishnu Teja and Rajagopalan, Vicram and Valmeekam, Chandra Shekhara
  Kaushik and Narayanan, Krishna and Chamberland, Jean-Francois and Kalathil, Dileep
  and Shakkottai, Srinivas
author:
- given: Vishnu Teja
  family: Kunde
- given: Vicram
  family: Rajagopalan
- given: Chandra Shekhara Kaushik
  family: Valmeekam
- given: Krishna
  family: Narayanan
- given: Jean-Francois
  family: Chamberland
- given: Dileep
  family: Kalathil
- given: Srinivas
  family: Shakkottai
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/kunde25a/kunde25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
