---
title: " Data-Driven Upper Confidence Bounds with Near-Optimal Regret for Heavy-Tailed
  Bandits "
openreview: " https://openreview.net/forum?id=8e9XlbzHbx "
abstract: " Stochastic multi-armed bandits (MABs) provide a fundamental reinforcement
  learning model to study sequential decision making in uncertain environments. The
  upper confidence bounds (UCB) algorithm gave birth to the renaissance of bandit
  algorithms, as it achieves near-optimal regret rates under various moment assumptions.
  Up until recently most UCB methods relied on concentration inequalities leading
  to confidence bounds which depend on moment parameters, such as the variance proxy,
  that are usually unknown in practice. In this paper, we propose a new distribution-free,
  data-driven UCB algorithm for symmetric reward distributions, which needs no moment
  information. The key idea is to combine a refined, one-sided version of the recently
  developed resampled median-of-means (RMM) method with UCB. We prove a near-optimal
  regret bound for the proposed anytime, parameter-free RMM-UCB method, even for heavy-tailed
  distributions. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tamas25a
month: 0
tex_title: " Data-Driven Upper Confidence Bounds with Near-Optimal Regret for Heavy-Tailed
  Bandits "
firstpage: 2116
lastpage: 2124
page: 2116-2124
order: 2116
cycles: false
bibtex_author: Tam{\'a}s, Ambrus and Szentp{\'e}teri, Szabolcs and Cs{\'a}ji, Bal{\'a}zs
author:
- given: Ambrus
  family: Tamás
- given: Szabolcs
  family: Szentpéteri
- given: Balázs
  family: Csáji
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/tamas25a/tamas25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
