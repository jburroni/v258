---
title: Optimizing Neural Network Training and Quantization with Rooted Logistic Objectives
software: https://github.com/ellenzhuwang/rooted_loss
openreview: g5ml9INmja
abstract: First-order methods are widely employed for training neural networks that
  are used in practical applications. For classification of input features, Cross-Entropy
  based loss functions are often preferred since they are differentiable everywhere.
  Recent optimization results show that the convergence properties of first-order
  methods such as gradient descent are intricately tied to the separability of datasets
  and the induced loss landscape. We introduce  Rooted Logistic Objectives (RLO) to
  improve practical convergence behavior with benefits for downstream tasks. We show
  that our proposed loss satisfies strict convexity properties and has better condition
  number properties that will benefit practical implementations. To evaluate our proposed
  RLO, we compare its performance on various classification benchmarks. Our results
  illustrate that training procedure converges faster with RLO in many cases. Furthermore,
  on two downstream tasks viz., post-training quantization and finetuning on quantized
  space, we show that it is possible to ensure lower performance degradation while
  using reduced precision for sequence prediction tasks in large language models over
  state of the art methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang25c
month: 0
tex_title: Optimizing Neural Network Training and Quantization with Rooted Logistic
  Objectives
firstpage: 982
lastpage: 990
page: 982-990
order: 982
cycles: false
bibtex_author: Wang, Zhu and Veluswami, Praveen Raj and Mishra, Harsh and Ravi, Sathya
  N.
author:
- given: Zhu
  family: Wang
- given: Praveen Raj
  family: Veluswami
- given: Harsh
  family: Mishra
- given: Sathya N.
  family: Ravi
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/wang25c/wang25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
