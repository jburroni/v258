---
title: 'Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn
  In-context by Multi-step Gradient Descent'
openreview: 4C1aRz2gRq
abstract: In-context learning has been recognized as a key factor in the success of
  Large Language Models (LLMs). It refers to the modelâ€™s ability to learn patterns
  on the fly from provided in-context examples in the prompt during inference. Previous
  studies have demonstrated that the Transformer architecture used in LLMs can implement
  a single-step gradient descent update by processing in-context examples in a single
  forward pass. Recent work has further shown that, during in-context learning, a
  looped Transformer can implement multi-step gradient descent updates in forward
  passes. However, their theoretical results require an exponential number of in-context
  examples, $n = \exp(\Omega(T))$, where $T$ is the number of loops or passes, to
  achieve a reasonably low error. In this paper, we study linear looped  Transformers
  in-context learning on linear vector generation tasks. We show that linear looped
  Transformers can implement multi-step gradient descent efficiently for in-context
  learning. Our results demonstrate that as long as the input data has a constant
  condition number, e.g., $n = O(d)$, the linear looped Transformers can achieve a
  small error by multi-step gradient descent during in-context learning. Furthermore,
  our preliminary experiments validate our theoretical analysis. Our findings reveal
  that the Transformer architecture possesses a stronger in-context learning capability
  than previously understood, offering new insights into the mechanisms behind LLMs
  and potentially guiding the better design of efficient inference algorithms for
  LLMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen25i
month: 0
tex_title: 'Bypassing the Exponential Dependency: Looped Transformers Efficiently
  Learn In-context by Multi-step Gradient Descent'
firstpage: 4447
lastpage: 4455
page: 4447-4455
order: 4447
cycles: false
bibtex_author: Chen, Bo and Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song,
  Zhao
author:
- given: Bo
  family: Chen
- given: Xiaoyu
  family: Li
- given: Yingyu
  family: Liang
- given: Zhenmei
  family: Shi
- given: Zhao
  family: Song
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/chen25i/chen25i.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
