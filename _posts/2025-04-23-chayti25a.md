---
title: Improving Stochastic Cubic Newton with Momentum
openreview: P63lweNABK
abstract: We study stochastic second-order methods for solving general non-convex
  optimization problems. We propose using a special version of momentum to stabilize
  the stochastic gradient and Hessian estimates in Newtonâ€™s method. We show that momentum
  provably improves the variance of stochastic estimates and allows the method to
  converge for any noise level. Using the cubic regularization technique, we prove
  a global convergence rate for our method on general non-convex problems to a second-order
  stationary point, even when using only a single stochastic data sample per iteration.
  This starkly contrasts with all existing stochastic second-order methods for non-convex
  problems, which typically require large batches. Therefore, we are the first to
  demonstrate global convergence for batches of arbitrary size in the non-convex case
  for the Stochastic Cubic Newton. Additionally, we show improved speed on convex
  stochastic problems for our regularized Newton methods with momentum.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chayti25a
month: 0
tex_title: Improving Stochastic Cubic Newton with Momentum
firstpage: 1441
lastpage: 1449
page: 1441-1449
order: 1441
cycles: false
bibtex_author: Chayti, El Mahdi and Doikov, Nikita and Jaggi, Martin
author:
- given: El Mahdi
  family: Chayti
- given: Nikita
  family: Doikov
- given: Martin
  family: Jaggi
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/chayti25a/chayti25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
