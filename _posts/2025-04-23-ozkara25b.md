---
title: 'Stochastic Rounding for LLM Training: Theory and Practice'
openreview: 3j3NtXcc95
abstract: As the parameters of Large Language Models (LLMs) have scaled to hundreds
  of billions, the demand for efficient training methods—balancing faster computation
  and reduced memory usage without sacrificing accuracy—has become more critical than
  ever. In recent years, various mixed precision strategies, which involve different
  precision levels for optimization components, have been proposed to increase training
  speed with minimal accuracy degradation. However, these strategies often require
  manual adjustments and lack theoretical justification. In this work, we leverage
  stochastic rounding (SR) to address numerical errors of training with low-precision
  representation. We provide theoretical analyses of implicit regularization and convergence
  under the Adam optimizer when SR is utilized. With the insights from these analyses,
  we extend previous BF16 + SR strategy to be used in distributed settings, enhancing
  the stability and performance for large scale training. Empirical results from pre-training
  models with up to 6.7B parameters, for the first time, demonstrate that our BF16
  with SR strategy outperforms (BF16, FP32) mixed precision strategies, achieving
  better validation perplexity, up to 1.54$\times$ higher throughput, and 30% lower
  memory usage.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ozkara25b
month: 0
tex_title: 'Stochastic Rounding for LLM Training: Theory and Practice'
firstpage: 4402
lastpage: 4410
page: 4402-4410
order: 4402
cycles: false
bibtex_author: Ozkara, Kaan and Yu, Tao and Park, Youngsuk
author:
- given: Kaan
  family: Ozkara
- given: Tao
  family: Yu
- given: Youngsuk
  family: Park
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/ozkara25b/ozkara25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
