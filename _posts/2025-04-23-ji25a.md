---
title: 'Rethinking Neural-based Matrix Inversion: Why can’t, and Where can'
software: https://github.com/yizhidamiaomiao/Neural-based-General-Matrix-Inversion-Why-can-t-and-Where-can
openreview: QTMczpOLMG
abstract: Deep neural networks have achieved substantial success across various scientific
  computing tasks. A pivotal challenge within this domain is the rapid and parallel
  approximation of matrix inverses, critical for numerous applications. Despite significant
  progress, there currently exists no universal neural-based method for approximating
  matrix inversion. This paper presents a theoretical analysis demonstrating the fundamental
  limitations of neural networks in developing a generalized matrix inversion model.
  We expand the class of Lipschitz functions to encompass a wider array of neural
  network models, thereby refining our theoretical approach. Moreover, we delineate
  specific conditions under which neural networks can effectively approximate matrix
  inverses. Our theoretical results are supported by experimental results from diverse
  matrix datasets, exploring the efficacy of neural networks in addressing the matrix
  inversion challenge.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ji25a
month: 0
tex_title: 'Rethinking Neural-based Matrix Inversion: Why can’t, and Where can'
firstpage: 3583
lastpage: 3591
page: 3583-3591
order: 3583
cycles: false
bibtex_author: Ji, Yuliang and Wu, Jian and Xi, Yuanzhe
author:
- given: Yuliang
  family: Ji
- given: Jian
  family: Wu
- given: Yuanzhe
  family: Xi
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/ji25a/ji25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
