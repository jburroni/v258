---
title: " Provable Benefits of Task-Specific Prompts for In-context Learning "
openreview: " https://openreview.net/forum?id=IWxPi5Ha7C "
abstract: " The in-context learning capabilities of modern language models have motivated
  a deeper mathematical understanding of sequence models. A line of recent work has
  shown that linear attention models can emulate projected gradient descent iterations
  to implicitly learn the task vector from the data provided in the context window.
  In this work, we consider a novel setting where the global task distribution can
  be partitioned into a union of conditional task distributions. We then examine the
  use of task-specific prompts and prediction heads for learning the prior information
  associated with the conditional task distribution using a one-layer attention model.
  Our results on loss landscape show that task-specific prompts facilitate a covariance-mean
  decoupling where prompt-tuning explains the conditional mean of the distribution
  whereas the variance is learned/explained through in-context learning. Incorporating
  task-specific head further aids this process by entirely decoupling estimation of
  mean and variance components. This covariance-mean perspective similarly explains
  how jointly training prompt and attention weights can provably help over fine-tuning
  after pretraining. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chang25b
month: 0
tex_title: " Provable Benefits of Task-Specific Prompts for In-context Learning "
firstpage: 1558
lastpage: 1566
page: 1558-1566
order: 1558
cycles: false
bibtex_author: Chang, Xiangyu and Li, Yingcong and Kara, Muti and Oymak, Samet and
  Roy-Chowdhury, Amit
author:
- given: Xiangyu
  family: Chang
- given: Yingcong
  family: Li
- given: Muti
  family: Kara
- given: Samet
  family: Oymak
- given: Amit
  family: Roy-Chowdhury
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/chang25b/chang25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
