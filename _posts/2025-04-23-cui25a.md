---
title: " Superiority of Multi-Head Attention: A Theoretical Study in Shallow Transformers
  in In-Context Linear Regression "
openreview: " https://openreview.net/forum?id=aUbv69Lbqt "
abstract: " We present a theoretical analysis of the performance of transformer with
  softmax attention in in-context learning with linear regression tasks. While the
  existing theoretical literature predominantly focuses on providing convergence upper
  bounds to show that trained transformers with single-/multi-head attention can obtain
  a good in-context learning performance, our research centers on comparing the exact
  convergence of single- and multi-head attention more rigorously. We conduct an exact
  theoretical analysis to demonstrate that multi-head attention with a substantial
  embedding dimension performs better than single-head attention. When the number
  of in-context examples $D$ increases, the prediction loss using single-/multi-head
  attention is in $O(1/D)$, and the one for multi-head attention has a smaller multiplicative
  constant. In addition to the simplest data distribution setting, our technical framework
  in calculating the exact convergence further facilitates studying more scenarios,
  e.g., noisy labels, local examples, correlated features, and prior knowledge. We
  observe that, in general, multi-head attention is preferred over single-head attention.
  Our results verify the effectiveness of the design of multi-head attention in the
  transformer architecture. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cui25a
month: 0
tex_title: " Superiority of Multi-Head Attention: A Theoretical Study in Shallow Transformers
  in In-Context Linear Regression "
firstpage: 937
lastpage: 945
page: 937-945
order: 937
cycles: false
bibtex_author: Cui, Yingqian and Ren, Jie and He, Pengfei and Liu, Hui and Tang, Jiliang
  and Xing, Yue
author:
- given: Yingqian
  family: Cui
- given: Jie
  family: Ren
- given: Pengfei
  family: He
- given: Hui
  family: Liu
- given: Jiliang
  family: Tang
- given: Yue
  family: Xing
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/cui25a/cui25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
