---
title: " Fundamental computational limits of weak learnability in high-dimensional
  multi-index models "
software: " https://github.com/SPOC-group/FundamentalLimitsMultiIndex "
openreview: " https://openreview.net/forum?id=Mwzui5H0VN "
abstract: " Multi-index models - functions which only depend on the covariates through
  a non-linear transformation of their projection on a subspace - are a useful benchmark
  for investigating feature learning with neural networks. This paper examines the
  theoretical boundaries of efficient learnability in this hypothesis class, focusing
  particularly on the minimum sample complexity required for weakly recovering their
  low-dimensional structure with first-order iterative algorithms, in the high-dimensional
  regime where the number of samples is $n=\\alpha d$ is proportional to the covariate
  dimension $d$. Our findings unfold in three parts: (i) first, we identify under
  which conditions a \\textit{trivial subspace} can be learned with a single step
  of a first-order algorithm for any $\\alpha>0$; (ii) second, in the case where the
  trivial subspace is empty, we provide necessary and sufficient conditions for the
  existence of an {\\it easy subspace} consisting of directions that can be learned
  only above a certain sample complexity $\\alpha>\\alpha_c$. The critical threshold
  $\\alpha_{c}$ marks the presence of a computational phase transition, in the sense
  that it is conjectured that no efficient iterative algorithm can succeed for $\\alpha<\\alpha_c$.
  In a limited but interesting set of really hard directions -akin to the parity problem-
  $\\alpha_c$ is found to diverge. Finally, (iii) we demonstrate that interactions
  between different directions can result in an intricate hierarchical learning phenomenon,
  where some directions can be learned sequentially when coupled to easier ones. Our
  analytical approach is built on the optimality of approximate message-passing algorithms
  among first-order iterative methods, delineating the fundamental learnability limit
  across a broad spectrum of algorithms, including neural networks trained with gradient
  descent. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: troiani25a
month: 0
tex_title: " Fundamental computational limits of weak learnability in high-dimensional
  multi-index models "
firstpage: 2467
lastpage: 2475
page: 2467-2475
order: 2467
cycles: false
bibtex_author: Troiani, Emanuele and Dandi, Yatin and Defilippis, Leonardo and Zdeborova,
  Lenka and Loureiro, Bruno and Krzakala, Florent
author:
- given: Emanuele
  family: Troiani
- given: Yatin
  family: Dandi
- given: Leonardo
  family: Defilippis
- given: Lenka
  family: Zdeborova
- given: Bruno
  family: Loureiro
- given: Florent
  family: Krzakala
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/troiani25a/troiani25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
