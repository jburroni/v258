---
title: Locally Optimal Descent for Dynamic Stepsize Scheduling
openreview: 2dQL1TBu5q
abstract: We introduce a novel dynamic learning-rate scheduling scheme grounded in
  theory with the goal of simplifying the manual and time-consuming tuning of schedules
  in practice.  Our approach is based on estimating the locally-optimal stepsize,
  guaranteeing maximal descent in the direction of the stochastic gradient of the
  current step.  We first establish theoretical convergence bounds for our method
  within the context of smooth non-convex stochastic optimization. We then present
  a practical implementation of our algorithm and conduct systematic experiments across
  diverse datasets and optimization algorithms, comparing our scheme with existing
  state-of-the-art learning-rate schedulers. Our findings indicate that our method
  needs minimal tuning when compared to existing approaches. Thus, removing the need
  for auxiliary manual schedules and warm-up phases and achieving comparable performance
  with drastically reduced parameter tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yehudai25a
month: 0
tex_title: Locally Optimal Descent for Dynamic Stepsize Scheduling
firstpage: 1099
lastpage: 1107
page: 1099-1107
order: 1099
cycles: false
bibtex_author: Yehudai, Gilad and Cohen, Alon and Daniely, Amit and Drori, Yoel and
  Koren, Tomer and Schain, Mariano
author:
- given: Gilad
  family: Yehudai
- given: Alon
  family: Cohen
- given: Amit
  family: Daniely
- given: Yoel
  family: Drori
- given: Tomer
  family: Koren
- given: Mariano
  family: Schain
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/yehudai25a/yehudai25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
