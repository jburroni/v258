---
title: Common Learning Constraints Alter Interpretations of Direct Preference Optimization
software: https://github.com/lmkong020/constraint-dpo-rlhf
openreview: UlrAxDAYqE
abstract: Large language models in the past have typically relied on some form of
  reinforcement learning with human feedback (RLHF) to better align model responses
  with human preferences.  However, because of oft-observed instabilities when implementing
  these RLHF pipelines, various reparameterization techniques have recently been introduced
  to sidestep the need for separately learning an RL reward model.  Instead, directly
  fine-tuning for human preferences is achieved via the minimization of a single closed-form
  training objective, a process originally referred to as direct preference optimization
  (DPO).  Although effective in certain real-world settings, we detail how the foundational
  role of DPO reparameterizations (and equivalency to applying RLHF with an optimal
  reward) may be obfuscated once inevitable optimization constraints are introduced
  during model training.  This then motivates alternative derivations and analysis
  of DPO that remain intact even in the presence of such constraints.  As initial
  steps in this direction, we re-derive DPO from a simple Gaussian estimation perspective,
  with strong ties to compressive sensing and classical constrained optimization problems
  involving noise-adaptive, concave regularization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kong25a
month: 0
tex_title: Common Learning Constraints Alter Interpretations of Direct Preference
  Optimization
firstpage: 2215
lastpage: 2223
page: 2215-2223
order: 2215
cycles: false
bibtex_author: Kong, Lemin and Hu, Xiangkun and He, Tong and Wipf, David
author:
- given: Lemin
  family: Kong
- given: Xiangkun
  family: Hu
- given: Tong
  family: He
- given: David
  family: Wipf
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/kong25a/kong25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
