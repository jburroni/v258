---
title: Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling
  Non-stationary Data
software: https://github.com/stlllll/TP-MOE
openreview: MpeO2sEww3
abstract: Mixture-of-expert (MOE) models are popular methods in machine learning,
  since they can model heterogeneous behaviour across the space of the data using
  an ensemble collection of learners. These models are especially useful for modelling
  dynamic data as time-dependent data often exhibit non-stationarity and heavy-tailed
  errors, which may be inappropriate to model with a typical single expert model.
  We propose a mixture of Student-$t$ processes with an adaptive structure for the
  covariance and noise behaviour for each mixture. Moreover, we use a sequential Monte
  Carlo (SMC) sampler to perform online inference as data arrive in real time. We
  demonstrate the superiority of our proposed approach over other models on synthetic
  and real-world datasets to prove the necessity of the novel method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sha25a
month: 0
tex_title: Online Student-$t$ Processes with an Overall-local Scale Structure for
  Modelling Non-stationary Data
firstpage: 1108
lastpage: 1116
page: 1108-1116
order: 1108
cycles: false
bibtex_author: Sha, Taole and Zhang, Michael Minyi
author:
- given: Taole
  family: Sha
- given: Michael Minyi
  family: Zhang
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/sha25a/sha25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
