---
title: 'The Size of Teachers as a Measure of Data Complexity: PAC-Bayes Excess Risk
  Bounds and Scaling Laws'
openreview: FDUfAcAVjO
abstract: We study the generalization properties of neural networks through the lens
  of data complexity.  Recent work by Buzaglo et al. (2024) shows that random (nearly)
  interpolating networks generalize, provided there is a small "teacher" network that
  achieves small excess risk.  We give a short single-sample PAC-Bayes proof of this
  result and an analogous "fast-rate" result for random samples from Gibbs posteriors.
  The resulting oracle inequality motivates a new notion of data complexity, based
  on the minimal size of a teacher network required to achieve any given level of
  excess risk. We show that polynomial data complexity gives rise to power laws connecting
  risk to the number of training samples, like in empirical neural scaling laws. By
  comparing the "scaling laws" resulting from our bounds to those observed in empirical
  studies, we provide evidence for lower bounds on the data complexity of standard
  benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dziugaite25a
month: 0
tex_title: 'The Size of Teachers as a Measure of Data Complexity: PAC-Bayes Excess
  Risk Bounds and Scaling Laws'
firstpage: 3979
lastpage: 3987
page: 3979-3987
order: 3979
cycles: false
bibtex_author: Dziugaite, Gintare Karolina and Roy, Daniel M.
author:
- given: Gintare Karolina
  family: Dziugaite
- given: Daniel M.
  family: Roy
date: 2025-04-23
address:
container-title: Proceedings of The 28th International Conference on Artificial Intelligence
  and Statistics
volume: '258'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 4
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v258/main/assets/dziugaite25a/dziugaite25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
